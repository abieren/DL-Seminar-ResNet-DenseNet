{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colab-Base-Config.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OndgzxVOwNPD","colab_type":"text"},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{"id":"UAMhJtZ1clZ7","colab_type":"text"},"source":["## Define custom implementations"]},{"cell_type":"code","metadata":{"id":"oXhwqtuhgZzF","colab_type":"code","colab":{}},"source":["import os\n","import shutil\n","\n","def read_label_from_txt_file(path):\n","  with open(path) as file:\n","    return file.readline()\n","\n","def read_labels_of_images(path):\n","  images = list(filter(lambda file_name: file_name.endswith(\".png\"), os.listdir(path)))\n","  labels = [read_label_from_txt_file(os.path.join(path, file_name[:-4] + \".txt\")) for file_name in images]\n","  return images, labels, set(labels)\n","\n","def sort_images_after_labels(src, dst):\n","  \"\"\"Sorts images in given source path into folders of their corresponding\n","     label. Folders of labels with the sorted images are places at the\n","     destination path.\"\"\"\n","  images, labels, label_set = read_labels_of_images(src)\n","  # create folder for each label\n","  for label in label_set:\n","    os.makedirs(os.path.join(dst, label), exist_ok=True)\n","  # copy images into label folders\n","  for image, label in zip(images, labels):\n","    shutil.copyfile(os.path.join(src, image), os.path.join(os.path.join(dst, label), image))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-Ed8GRKjqwV","colab_type":"code","colab":{}},"source":["class PathConfig:\n","  \"\"\"Holds the path configuration for training.\"\"\"\n","\n","  def __init__(self,\n","               model_path,            # directory of safed model\n","               model_file,            # file name of saved model\n","               session_path,          # directory of session object collecting the training data\n","               session_file,          # file name of session object\n","               train_path,            # directory of training data\n","               test_path              # directory of testing data\n","               ):\n","    self.model_path   = model_path\n","    self.model_file   = model_file\n","    self.session_path = session_path\n","    self.session_file = session_file\n","    self.train_path   = train_path\n","    self.test_path    = test_path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE0kI6Zul2Ey","colab_type":"code","colab":{}},"source":["class TrainingMode:\n","  \"\"\"Enum for specifing the traing mode.\"\"\"\n","\n","  # Train with no existing training session. \n","  # If a training session already exists, it is ignored and overwritten.\n","  NEW_TRAINING        = 0   \n","  \n","  # Tries to resume training with a existing training session.\n","  # If no existing session exists, a new one is created.\n","  TRY_RESUME_TRAINING = 1   \n","\n","  # Resumes training with a existing training session.\n","  # If no training session exists, an exception is thrown.\n","  RESUME_TRAINING     = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnYWTWmxkraZ","colab_type":"code","colab":{}},"source":["import pickle\n","\n","class TrainingSession:\n","  \"\"\"Holds and collects the training data that is produced during training.\"\"\"\n","  \n","  def __init__(self):\n","    self.super_epochs      = 0      # conter of super epochs\n","    self.total_epochs      = 0      # conter of all epochs\n","    self.training_history  = []     # list to aggregate the training history of each super epoch\n","    self.super_epoch_start = []     # list with timestamps when the super epoch started\n","    self.super_epoch_end   = []     # list with timestamps when the super epoch ended\n","\n","  def load(path):\n","    with open(path, \"rb\") as file:\n","      return pickle.load(file)\n","\n","  def save(self, path):\n","    with open(path , \"wb\") as file:\n","      pickle.dump(self, file)\n","\n","  def append(self, num_epochs, history, start, end):\n","    self.super_epochs      = self.super_epochs  + 1\n","    self.total_epochs      = self.total_epochs  + num_epochs\n","    self.training_history  = self.training_history  + [history]\n","    self.super_epoch_start = self.super_epoch_start + [start]\n","    self.super_epoch_end   = self.super_epoch_end   + [end]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wj1J-kvWu9pz","colab_type":"code","colab":{}},"source":["import os\n","\n","class TrainingSessionProvider:\n","  \"\"\"Provides a TrainingSession object by loading a existing or\n","     creating a new one.\"\"\"\n","     \n","  def provide(self, path_config, training_mode):\n","    if training_mode is TrainingMode.NEW_TRAINING:\n","      return self._create_session()\n","\n","    elif training_mode is TrainingMode.RESUME_TRAINING:\n","      path = os.path.join(path_config.session_path, path_config.session_file)\n","      return self._load_session(path)\n","\n","    elif training_mode is TrainingMode.TRY_RESUME_TRAINING:\n","      path = os.path.join(path_config.session_path, path_config.session_file)\n","      return self._try_load_session_otherwise_create(path)\n","    \n","    else:\n","      raise ValueError(\"Unexpected enum value: %d\" % training_mode)\n","\n","  def _try_load_session_otherwise_create(self, path):\n","    session = None\n","    print(\"Try loading session\")\n","    try:\n","      session = self._load_session(path)\n","    except FileNotFoundError:\n","      print(\"Session not found\")\n","      session = self._create_session()\n","    return session\n","\n","  def _load_session(self, path):\n","    print(\"Loading session from:\", path)\n","    return TrainingSession.load(path)\n","\n","  def _create_session(self):\n","    print(\"Creating new session\")\n","    return TrainingSession()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBVviKsNBzy6","colab_type":"code","colab":{}},"source":["from abc import ABC, abstractmethod\n","import tensorflow as tf\n","\n","class AbstractModelProvider(ABC):\n","  \"\"\"Model provider that loads a existing model or creates a new one.\"\"\"\n","\n","  def provide(self, path_config, training_mode):\n","    if training_mode is TrainingMode.NEW_TRAINING:\n","      return self._create_model()\n","\n","    elif training_mode is TrainingMode.RESUME_TRAINING:\n","      path = os.path.join(path_config.model_path, path_config.model_file)\n","      return self._load_model(path)\n","\n","    elif training_mode is TrainingMode.TRY_RESUME_TRAINING:\n","      path = os.path.join(path_config.model_path, path_config.model_file)\n","      return self._try_load_model_otherwise_create(path)\n","    \n","    else:\n","      raise ValueError(\"Unexpected enum value: %d\" % training_mode)\n","\n","  def _try_load_model_otherwise_create(self, path):\n","    model = None\n","    print(\"Try loading model\")\n","    try:\n","      model = self._load_model(path, self._supply_custom_objects())\n","    except OSError:\n","      print(\"Model not found\")\n","      model = self._create_model()\n","    return model\n","\n","  def _load_model(self, path, custom_objects):\n","    print(\"Loading model from:\", path)\n","    return tf.keras.models.load_model(\n","        path,\n","        custom_objects=custom_objects, \n","        compile=False)                    # model is recompiled before training\n","\n","  def _supply_custom_objects(self):\n","    \"\"\"Override this method to define custom objects for keras when loading an\n","       existing model, since custom objects are not saved to the model file.\"\"\"\n","    return None\n","\n","  @abstractmethod\n","  def _create_model(self):\n","    \"\"\"Abstract method to create the model.\"\"\"\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctXfpivXjosA","colab_type":"code","colab":{}},"source":["class TrainingConfig:\n","  \"\"\"Holds the training configuration for the Trainer object.\"\"\"\n","\n","  def __init__(self,\n","               optimizer,             # optimizer for training the model\n","               loss_function,         # loss function to maximize\n","               metrics,               # metrics to calculate\n","               train_batch_size,      # batch size for training data\n","               test_batch_size,       # batch size for testing data\n","               epochs,                # number of epochs\n","               super_epochs           # number of super epochs, total epochs = super_epochs * epochs\n","               ):\n","    self.optimizer        = optimizer\n","    self.loss_function    = loss_function\n","    self.metrics          = metrics\n","    self.train_batch_size = train_batch_size\n","    self.test_batch_size  = test_batch_size\n","    self.epochs           = epochs\n","    self.super_epochs     = super_epochs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMkTndN011z9","colab_type":"code","colab":{}},"source":["import os\n","import datetime\n","\n","class Trainer:\n","  \"\"\"Executes the training of a model and also saves checkpoints of the trained\n","     model and traing session.\"\"\"\n","\n","  def __init__(self, training_session, model, path_config, training_config):\n","    self.training_session      = training_session\n","    self.model                 = model\n","    self.path_config           = path_config\n","    self.training_config       = training_config\n","    self._num_train_samples    = None\n","    self._num_test_samples     = None\n","    self._train_data_generator = None\n","    self._test_data_generator  = None\n","    self._checkpoint_callback  = None\n","\n","  def _prepare_training(self):\n","    # get width and height of input layer\n","    INPUT_SIZE = self.model.input_shape[1:3]\n","\n","    # get number of training samples\n","    self._num_train_samples = self._count_samples(self.path_config.train_path)\n","\n","    # get number of test samples\n","    self._num_test_samples = self._count_samples(self.path_config.test_path)\n","\n","    # generator for training data\n","    self._train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n","        self.path_config.train_path,\n","        target_size   = INPUT_SIZE,\n","        batch_size    = self.training_config.train_batch_size,\n","        interpolation = \"bilinear\")\n","\n","    # generator for test data\n","    self._test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n","        self.path_config.test_path,\n","        target_size   = INPUT_SIZE,\n","        batch_size    = self.training_config.test_batch_size,\n","        interpolation = \"bilinear\")\n","\n","    # create folder for model checkpoints\n","    os.makedirs(self.path_config.model_path, exist_ok=True)\n","\n","    # define callback to save the model\n","    self._checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath          = os.path.join(self.path_config.model_path, self.path_config.model_file),\n","        monitor           = 'val_accuracy', \n","        verbose           = 1, \n","        save_best_only    = False,\n","        save_weights_only = False,\n","        mode              = 'max')\n","\n","    # (re)compile model with specified optimizer, loss funtion and metrics\n","    self.model.compile(\n","        optimizer = self.training_config.optimizer, \n","        loss      = self.training_config.loss_function, \n","        metrics   = self.training_config.metrics)\n","    \n","  def _count_samples(self, path):\n","    count = 0\n","    for category in os.listdir(path):\n","      count = count + len(os.listdir(os.path.join(path, category)))\n","    return count\n","\n","  def _get_mins_and_secs_from_secs(self, seconds):\n","    mins = seconds // 60\n","    secs = seconds %  60\n","    return (mins, secs)\n","    \n","  def _execute_training(self):\n","    try:\n","      while self.training_session.super_epochs < self.training_config.super_epochs:\n","        print(\"Training super epoch %d of %d\" \n","              % (self.training_session.super_epochs + 1, self.training_config.super_epochs))\n","\n","        start = datetime.datetime.now()\n","        history = self.model.fit_generator(\n","                generator        = self._train_data_generator,\n","                steps_per_epoch  = self._num_train_samples // self.training_config.train_batch_size,\n","                epochs           = self.training_config.epochs,\n","                callbacks        = [self._checkpoint_callback],\n","                validation_data  = self._test_data_generator,\n","                validation_steps = self._num_test_samples // self.training_config.test_batch_size)\n","        end = datetime.datetime.now()\n","\n","        self.training_session.append(\n","            num_epochs = self.training_config.epochs, \n","            history    = history.history,\n","            start      = start,\n","            end        = end)   \n","        self.training_session.save(\n","            os.path.join(self.path_config.session_path, self.path_config.session_file))\n","        \n","        print(\"Finished super epoch within %02d:%02d (mins:secs)\" \n","              % self._get_mins_and_secs_from_secs((end - start).total_seconds()))\n","      \n","      print(\"Done with training %d super epochs\" % self.training_session.super_epochs)\n","    except KeyboardInterrupt:\n","      print(\"\\nTraining aborted by user\")\n","    \n","  def train(self):\n","    self._prepare_training()\n","    self._execute_training()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7sghWXQdYaLD","colab_type":"text"},"source":["## Configuration"]},{"cell_type":"markdown","metadata":{"id":"JJ-3dTjfZ30Z","colab_type":"text"},"source":["### Path variables"]},{"cell_type":"code","metadata":{"id":"jUB-uoSwJK-O","colab_type":"code","colab":{}},"source":["#@title Set colab path in Google Drive\n","\n","#@markdown Path to the folder where colab should read data from and write to\n","GDRIVE_COLAB_PATH       = \"colab\"           # @param {type: \"string\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXkjxfigFW6r","colab_type":"code","colab":{}},"source":["#@title Set experiment group and experiment name\n","#@markdown The output path for this experiment is determined by the following two variables.\n","\n","#@markdown Name of the experiment group this experiment belongs to\n","EXPERIMENT_GROUP        = \"Default\"   # @param {type: \"string\"}\n","\n","#@markdown Name of this experiment\n","EXPERIMENT_NAME         = \"ResNet-50\"      # @param {type: \"string\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJglmKdClqnn","colab_type":"code","colab":{}},"source":["import os\n","\n","# basic paths\n","GDRIVE_MOUNTING_PATH    = \"/gdrive\"                                             # path where the google drive will be mounted\n","                                                                                # remote path of colab data\n","REMOTE_DATA_PATH        = os.path.join(GDRIVE_MOUNTING_PATH, \"My Drive\", GDRIVE_COLAB_PATH)  \n","LOCAL_DATA_PATH         = \"/home/data\"                                          # local path of colab data\n","REMOTE_ROBOCUP          = os.path.join(REMOTE_DATA_PATH, \"Robo Cup\")            # remote path of robo cup data\n","LOCAL_ROBOCUP           = os.path.join(LOCAL_DATA_PATH,  \"Robo Cup\")            # local path of robo cup data\n","\n","# required paths for data mangling\n","TRAIN_PATH_UNSORTED     = os.path.join(LOCAL_ROBOCUP, \"train\")                  # path of train data before data reorganization\n","TEST_PATH_UNSORTED      = os.path.join(LOCAL_ROBOCUP, \"test\")                   # path of test data before data reorganization\n","\n","# required paths for training\n","TRAIN_PATH_SORTED       = os.path.join(LOCAL_ROBOCUP, \"train_sorted\")           # path of train data after data reorganization\n","TEST_PATH_SORTED        = os.path.join(LOCAL_ROBOCUP, \"test_sorted\")            # path of test data after data reorganization\n","\n","REMOTE_OUTPUT_PATH      = os.path.join(REMOTE_DATA_PATH, \"output\")              # path where output of colab is written to or read from\n","                                                                                # used for saving the training session, model and analysis\n","                                                                            \n","                                                                                # path where the output of the experiment is written to\n","REMOTE_OUTPUT_EXPERIMENT_PATH = os.path.join(REMOTE_OUTPUT_PATH, EXPERIMENT_GROUP, EXPERIMENT_NAME)                             \n","                                                                            \n","                                                                                # path where the analysis results are written to\n","REMOTE_OUTPUT_ANALYSIS_PATH   = os.path.join(REMOTE_OUTPUT_EXPERIMENT_PATH, \"analysis\")\n","                        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0rbKe3JbPrP","colab_type":"text"},"source":["### Path configuration for training"]},{"cell_type":"code","metadata":{"id":"KHGi3OrtkAOR","colab_type":"code","colab":{}},"source":["PATH_CONFIG = PathConfig(\n","    model_path   = REMOTE_OUTPUT_EXPERIMENT_PATH,\n","    model_file   = \"RoboCup-model.hdf5\", \n","    session_path = REMOTE_OUTPUT_EXPERIMENT_PATH, \n","    session_file = \"session.pickle\",\n","    train_path   = TRAIN_PATH_SORTED, \n","    test_path    = TEST_PATH_SORTED)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huuVxm0Am3YU","colab_type":"text"},"source":["### Training mode"]},{"cell_type":"code","metadata":{"id":"VNqS9ByNms2N","colab_type":"code","colab":{}},"source":["TRAINING_MODE = TrainingMode.TRY_RESUME_TRAINING"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRyqnTSRnJpo","colab_type":"text"},"source":["### Training config"]},{"cell_type":"code","metadata":{"id":"MOrUapgEvsO6","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","TRAINING_CONFIG = TrainingConfig(\n","    optimizer        = tf.keras.optimizers.Adam(), \n","    loss_function    = tf.keras.losses.categorical_crossentropy, \n","    metrics          = [tf.keras.metrics.Accuracy(), tf.keras.metrics.CategoricalAccuracy()],\n","    train_batch_size = 10, \n","    test_batch_size  = 10, \n","    epochs           = 1, \n","    super_epochs     = 50 )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUJbh5NLpD0G","colab_type":"text"},"source":["### Model provider"]},{"cell_type":"code","metadata":{"id":"AvOfKGSGpDBO","colab_type":"code","colab":{}},"source":["class ModelProvider(AbstractModelProvider):\n","  def _create_model(self):\n","    print(\"Creating new model\")\n","    # get model with pre trained weights\n","    model = tf.keras.applications.ResNet50(weights=\"imagenet\")\n","    # replace last softmax layer from ImageNet domain with new soft max layer for our RoboCup domain \n","    x = tf.keras.layers.Dense(8, activation='softmax', name='predictions')(model.layers[-2].output)\n","    # create new model\n","    return tf.keras.models.Model(inputs=model.input, outputs=x)\n","\n","MODEL_PROVIDER = ModelProvider()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JSIvXKQwYWT","colab_type":"text"},"source":["## Execute experiment"]},{"cell_type":"markdown","metadata":{"id":"BQbYw_ohJBOg","colab_type":"text"},"source":["### Download train and test data from Google Drive"]},{"cell_type":"markdown","metadata":{"id":"tPOXtlYo3fPV","colab_type":"text"},"source":["Authenticate and mount Google Drive"]},{"cell_type":"code","metadata":{"id":"4SOaYYpl0N65","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount(GDRIVE_MOUNTING_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXwpBmtzBtKu","colab_type":"text"},"source":["### Copy RoboCup data from Google Drive to machine"]},{"cell_type":"code","metadata":{"id":"QcNJTKT82Ymh","colab_type":"code","colab":{}},"source":["SRC = \"'\" + REMOTE_ROBOCUP + \"'\"\n","DST = \"'\" + LOCAL_ROBOCUP  + \"'\"\n","!mkdir -p $DST\n","!cp -r $SRC/. $DST"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nKfo8vTWB47g","colab_type":"text"},"source":["### Unzip train and test data"]},{"cell_type":"code","metadata":{"id":"Z7vGnnoZAhD1","colab_type":"code","colab":{}},"source":["!unzip $DST/train.zip -d $DST\n","!unzip $DST/test.zip -d $DST "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFB9hSKlx_bL","colab_type":"text"},"source":["### Reorganize train and test data"]},{"cell_type":"markdown","metadata":{"id":"_a2lveekzU1R","colab_type":"text"},"source":["Group test and train images after their corresponding labels\n"]},{"cell_type":"code","metadata":{"id":"p9KZl4pkvlNI","colab_type":"code","colab":{}},"source":["sort_images_after_labels(TRAIN_PATH_UNSORTED, TRAIN_PATH_SORTED)\n","sort_images_after_labels(TEST_PATH_UNSORTED,  TEST_PATH_SORTED)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcZRpOR3yOZ2","colab_type":"text"},"source":["### Load or create training session"]},{"cell_type":"code","metadata":{"id":"1kNU18dVr9_q","colab_type":"code","colab":{}},"source":["TRAINING_SESSION = TrainingSessionProvider().provide(PATH_CONFIG, TRAINING_MODE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1uL_CuXQgi3","colab_type":"text"},"source":["### Load or create model"]},{"cell_type":"code","metadata":{"id":"R3QtB8HQMSi1","colab_type":"code","colab":{}},"source":["MODEL = MODEL_PROVIDER.provide(PATH_CONFIG, TRAINING_MODE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HngKlP0I9fyS","colab_type":"text"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"dNFo5otdxSsK","colab_type":"code","colab":{}},"source":["TRAINER = Trainer(\n","    training_session = TRAINING_SESSION,\n","    model            = MODEL,\n","    path_config      = PATH_CONFIG, \n","    training_config  = TRAINING_CONFIG)\n","\n","TRAINER.train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk6TbIIGveVj","colab_type":"text"},"source":["# Experiment analysis"]},{"cell_type":"markdown","metadata":{"id":"PfQqyqIl7Zzv","colab_type":"text"},"source":["## Define custom implementations"]},{"cell_type":"code","metadata":{"id":"W_HhUSSKAeNy","colab_type":"code","colab":{}},"source":["import os\n","\n","def files_in_dir(path):\n","  return [os.path.join(path, file_name) for file_name in os.listdir(path)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDFOa-yS7tZz","colab_type":"code","colab":{}},"source":["def merge_histories(old, new):\n","  result = old.copy()\n","  for k, v in new.items():\n","    if k not in result:\n","      result[k] = []\n","    result[k] = [*result[k], *new[k]]\n","  return result\n","\n","def merge_all_histories(histories):\n","  entire_history = {}\n","  for history in histories:\n","    entire_history = merge_histories(entire_history, history)\n","  return entire_history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrT9_It4lElP","colab_type":"code","colab":{}},"source":["def training_seconds(session_data):\n","  sum = 0\n","  for start, end in zip(session_data.super_epoch_start, session_data.super_epoch_end):\n","    sum = sum + (end - start).total_seconds()\n","  return sum"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsTPc0dbSnZG","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","def load_image(path):\n","  INPUT_SIZE = MODEL.input_shape[1:3]\n","  return tf.keras.preprocessing.image.load_img(path, target_size=INPUT_SIZE)\n","\n","def test_image(model, path):\n","  INPUT_SIZE = model.input_shape[1:3]\n","  img = tf.keras.preprocessing.image.load_img(os.path.join(TEST_PATH_SORTED, path), target_size=INPUT_SIZE)\n","  img_as_array = tf.keras.preprocessing.image.img_to_array(img)\n","  prediction = model.predict(np.array([img_as_array]))\n","  print(\"probabilities:\", prediction)\n","  print(\"argmax:\", np.argmax(prediction))\n","  plt.imshow(img)\n","\n","def predict_image_probs(model, path):\n","  INPUT_SIZE = model.input_shape[1:3]\n","  img = tf.keras.preprocessing.image.load_img(os.path.join(TEST_PATH_SORTED, path), target_size=INPUT_SIZE)\n","  img_as_array = tf.keras.preprocessing.image.img_to_array(img)\n","  return model.predict(np.array([img_as_array]))\n","\n","def predict_image_argmax(model, path):\n","  return np.argmax(predict_image_probs(model, path))\n","\n","def predict_all_test_images(model, test_path_sorted):\n","  predictions = [(\n","      predict_image_argmax(model, os.path.join(test_path_sorted, label, image)),\n","      int(label), \n","      image, \n","      os.path.join(test_path_sorted, label, image))\n","    for label in os.listdir(test_path_sorted)\n","    for image in os.listdir(os.path.join(test_path_sorted, label))]\n","  \n","  # split up list\n","  pred, label, image, path = zip(*predictions) \n","  return np.array(pred), np.array(label), np.array(image), np.array(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zLDsXWE7NDF","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","  # Plot training & validation accuracy values\n","  plt.plot(history['accuracy'])\n","  plt.plot(history['val_accuracy'])\n","  plt.title('Model accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'], loc='upper left')\n","  plt.show()\n","\n","  # Plot training & validation categorical accuracy values\n","  plt.plot(history['categorical_accuracy'])\n","  plt.plot(history['val_categorical_accuracy'])\n","  plt.title('Model categorical accuracy')\n","  plt.ylabel('Categorical Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'], loc='upper left')\n","  plt.show()\n","\n","  # Plot training & validation loss values\n","  plt.plot(history['loss'])\n","  plt.plot(history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'], loc='upper left')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lf44ZiF9n2c","colab_type":"code","colab":{}},"source":["def find_falsely_classified_images(preds, labels, images, paths):\n","  mask = preds != labels\n","  false_preds  = preds[mask]\n","  false_labels = labels[mask]\n","  false_images = images[mask]\n","  false_paths  = paths[mask]\n","\n","  return (false_preds, false_labels, false_images, false_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LR61oMZfEqa4","colab_type":"code","colab":{}},"source":["def plot_falsely_classified_images(false_preds, false_labels, false_images, false_paths):\n","  translation = {0: \"Ball\",\n","                1: \"Goal post\", \n","                2: \"Obstacle\", \n","                3: \"L-Line\",\n","                4: \"X-Line\",\n","                5: \"T-Line\",\n","                6: \"Penalty spot\",\n","                7: \"Robot foot\"}\n","\n","  rows = len(false_images) + 1      # +1 for heading row\n","  cols = 3\n","  fig = plt.figure(figsize=(8, 3*rows))\n","\n","  fig.add_subplot(rows, cols, 1)\n","  plt.axis('off')\n","  plt.text(0, 0.5, \"Bild\", fontsize=14)\n","\n","  fig.add_subplot(rows, cols, 2)\n","  plt.axis('off')\n","  plt.text(0, 0.5, \"Vorhersage\", fontsize=14)\n","\n","  fig.add_subplot(rows, cols, 3)\n","  plt.axis('off')\n","  plt.text(0, 0.5, \"Label\", fontsize=14)\n","\n","  for i in range(1,rows):\n","      img = load_image(false_paths[i-1])\n","      # create subplot for image\n","      fig.add_subplot(rows, cols, i*3+1)\n","      plt.axis('off')\n","      #plt.tight_layout()\n","      plt.imshow(img)\n","      # create subplot for prediction\n","      fig.add_subplot(rows, cols, i*3+2)\n","      plt.axis('off')\n","      #plt.tight_layout()\n","      plt.text(0, 0.5, translation[false_preds[i-1]], fontsize=14)\n","      # create subplot for label\n","      fig.add_subplot(rows, cols, i*3+3)\n","      plt.axis('off')\n","      #plt.tight_layout()\n","      plt.text(0, 0.5, translation[false_labels[i-1]], fontsize=14)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pTotfi6JyJs6","colab_type":"text"},"source":["## Execute analysis"]},{"cell_type":"markdown","metadata":{"id":"5WdJGixPyPdu","colab_type":"text"},"source":["### Calculate training time"]},{"cell_type":"code","metadata":{"id":"ZuAV8IMwl5h8","colab_type":"code","colab":{}},"source":["training_seconds(TRAINING_SESSION) / 60   # minutes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sinzlMyycDX","colab_type":"text"},"source":["### Plot training history"]},{"cell_type":"code","metadata":{"id":"6JX4bCat_qnw","colab_type":"code","colab":{}},"source":["plot_history(merge_all_histories(TRAINING_SESSION.training_history))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ANv3C6MT9zQS","colab_type":"text"},"source":["### Predict all images"]},{"cell_type":"code","metadata":{"id":"w_zweqqV9ynH","colab_type":"code","colab":{}},"source":["preds, labels, images, paths = predict_all_test_images(MODEL, TEST_PATH_SORTED)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZBrnkp_zNtE","colab_type":"text"},"source":["### Plot confusion matrix"]},{"cell_type":"code","metadata":{"id":"cBJOUGzB5oHZ","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","import os\n","import seaborn as sn\n","\n","confusion = confusion_matrix(labels, preds)\n","\n","sn.heatmap(confusion)\n","plt.show()\n","\n","confusion"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03XmZw9H-H1E","colab_type":"text"},"source":["### Find all falsely classified images"]},{"cell_type":"code","metadata":{"id":"1Heu52Rg-N_U","colab_type":"code","colab":{}},"source":["false_preds, false_labels, false_images, false_paths = find_falsely_classified_images(preds, labels, images, paths)\n","\n","for i in range(0, len(false_preds)):\n","  print(\"label(%d) prediction(%d) %s %s\" % (false_labels[i], false_preds[i], false_images[i], false_paths[i]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6VBNh68QTJ7","colab_type":"text"},"source":["### Plot wrongly classified images\n"]},{"cell_type":"code","metadata":{"id":"I4Wkhr_VKXOI","colab_type":"code","colab":{}},"source":["plot_falsely_classified_images(false_preds, false_labels, false_images, false_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knfQOmEQZctH","colab_type":"text"},"source":["### Test single images"]},{"cell_type":"code","metadata":{"id":"wns2zeYuTMkb","colab_type":"code","colab":{}},"source":["# test a particular image\n","test_image(MODEL, os.path.join(TEST_PATH_SORTED, \"1/1761.png\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKvrvcRmzHGO","colab_type":"text"},"source":["### Visualize model"]},{"cell_type":"code","metadata":{"id":"x13fKODxCipG","colab_type":"code","colab":{}},"source":["# visualize model\n","tf.keras.utils.plot_model(MODEL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_gnkyDKHqJtg","colab_type":"text"},"source":["# Run TensorBoard (this notebook does not yet track something with TensorBoard)\n","\n","Tutorial how to run TensorBoard in Cloab from:\n","[https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/](https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/)\n"]},{"cell_type":"code","metadata":{"id":"xAsB_j8ezmXo","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwUMu1ZWqK8A","colab_type":"code","colab":{}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwkULhgwqaGJ","colab_type":"code","colab":{}},"source":["LOG_DIR = './log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TfAHvsIqigS","colab_type":"code","colab":{}},"source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_l9l1pypq8Ww","colab_type":"code","colab":{}},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]}]}